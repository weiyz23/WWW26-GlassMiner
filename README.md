# GlassMiner: Mining Looking Glass Services via Structure-Semantics Fusion for Web Observability

## Introduction

GlassMiner is a framework designed to mine and analyze the Looking Glass (LG) services on the Internet. It consists of four main modules:

- **Template clustering analysis**: This module processes the seed pages of LG services and performs clustering analysis by fused structural and semantic features, and then performs selective TF-IDF analysis to extract the most representative keywords for each cluster.

- **Network-aware crawler**: This module implements a network-aware crawler to collect webpages of LG services. It uses the clustering results from the previous module and the AS (Autonomous System) information to guide the crawling process. The crawler is designed to be efficient and effective in collecting relevant webpages.

- **LLM-driven classification**: This module uses large language models (LLMs) to classify the webpages collected by the crawler. It employs a few-shot learning approach to improve classification accuracy and reduce the need for extensive labeled data.

- **LG VP discovery**: This module implements a distributed system to discover the VPs of LG services and confirm their availability and API functionality. It uses a set of templates to identify the VPs and employs a distributed approach to improve efficiency.

## Result List

You can find the result list of our paper [here](https://weiyz23.github.io/WWW26-GlassMiner/country/).

## How to use

The GlassMiner framework is designed to be modular, allowing users to run each component independently.
We have made each module **executable in sequence**, and below we will explain how to use each module.

### 0. Environment Setup

GlassMiner is implemented in Python 3.11. You can create a virtual environment, then install the required packages using the following commands:

```bash
# Create a virtual environment
conda create -n glassminer python=3.11
# Activate the virtual environment
conda activate glassminer
# Install the required packages
pip install -r requirements.txt
```

You also need to install `google-chrome-stable` for the crawler to work properly. You can follow the instructions [here](https://www.google.com/chrome/) to install it.

```bash
wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo gpg --dearmor -o /usr/share/keyrings/google-linux-signing-keyring.gpg
echo "deb [arch=amd64 signed-by=/usr/share/keyrings/google-linux-signing-keyring.gpg] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
sudo apt update
sudo apt install google-chrome-stable
```

We have placed some necessary files in the `/shared_data` directory, they come from public datasets or are generated by ourselves using the scripts in this repository. You can replace them with your own data and outputs if needed.

### 1. Seedpage processing and clustering

This part corresponds to the logic in the subdirectory `seed_pages`, including seed page processing and **template clustering analysis**.

We have placed some necessary files in the `/shared_data` directory, and you can directly execute the files in the `/src` subdirectory in sequential order.
After executing, you will find the corresponding clustering results and corpus analysis results in the `/output` subdirectory.


### 2. Webpage crawling

This part corresponds to the logic in the subdirectory `webpage_crawler`, including the **network-aware crawler**.

The input of this module mainly includes the output of the previous module and some necessary AS-related data, we have placed all the necessary files in the `/shared_data` directory, and you can directly execute the files in the `/src` subdirectory in sequential order.

### 3. LLM-driven Classification

This part corresponds to the logic in the subdirectory `llm_classifier`, including the **LLM-driven classification** module.

> **Preliminary**:
>
> To use the paid LLM service, you need to fill in your personal API Key in `/src/utils.py` to use the [silicon-based flow LLM service](https://cloud.siliconflow.cn/account/ak):
>
> ```python
> API_HEADER = {
>     "Authorization": "Bearer <Your API Key>",
>     "Content-Type": "application/json"
> }
> ```
> Of course, you can also connect to other LLM models that you have deployed yourself!


We have provided the selected prompts, so you can skip running `/src/2_autoselection.py`.
If you want to build this part yourself, you can run `/src/2_autoselection.py` to select the optimal few-shot samples.


You may need to manually trim the content of the few-shot samples after running `/src/2_autoselection.py` to reduce inference costs, and update it in the `prompted_binary_classification()` function in `/src/utils.py`.


### 4. VP Discovery

This part corresponds to the logic in the subdirectory `vp_discovery`, including the **LG VP discovery** module.

> **Preliminary**:
>
> You need to replace the machine IP addresses in the `/src/configs.py` file with your own machine IP address (and the usernames and the file paths). We strongly recommend using at least 3 machines to run this module.
>
> ```python
> HOSTS = [
>     {
>         "public_ip": "<Your Public IP #3>",
>         "private_ip": "<Your Private IP #3>",
>         "username": "<Your SSH Username #3>",
>         "port": 22,
>         # "passwd": "",  # Optional, if using password authentication
>         "pcap_path": "/tmp/2_receive.pcap",
>         "local_path": "2_receive.pcap",
>     },
>     ...
> ]
> ```

### 5. lg_prober (Experimental)

Here we provide a simple tool `lg_prober` to help you quickly check the availability of LG services and their VPs.
We have currently implemented an automated `traceroute` measurement task invocation and result parsing tool, but it is still in the experimental stage, please use it with caution.

**Note**: 
1. We have manually processed the templates in `/src/templates.py`, but we have not checked all the templates as it is a time-consuming task. We only checked and reported the more frequent templates in the paper. We believe that **there are actually more automatable templates**, and you can modify them according to your needs.
2. We discard those LG services that only provide BGP summary queries and do not provide any other valuable services. They can be classified as LG services, but we just ignore them in successive steps. If you want to include them, you can parse and add more logic in the `/src/templates.py` file.